base_model: Qwen/Qwen3-4B-Instruct-2507
trust_remote_code: true

# Quantization (QLoRA 4-bit)
load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
bnb_4bit_compute_dtype: bfloat16

# Use Qwen3 chat template to render messages
chat_template: qwen3

datasets:
  - path: ./prepared_train.jsonl
    type: chat_template
    split: train
    field_messages: messages
    message_property_mappings:
      role: role
      content: content
val_set_size: 0

# Sequence/prompt length
sequence_len: 32768 # I don't think we can go higher than 16384
pack: false

train_on_inputs: false

# LoRA
adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
merge_lora: true
save_safetensors: true

# Use the Axolotl/PEFT key that is actually read:
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
# If this still fails, comment the above and use the broad fallback:
# lora_target_linear: true


# Optim / schedule
optimizer: paged_adamw_8bit
learning_rate: 2e-4
lr_scheduler: cosine
warmup_ratio: 0.03
weight_decay: 0.0

# Batching
micro_batch_size: 1
gradient_accumulation_steps: 8
num_epochs: 2

# Precision
bf16: true
# fp16: true  # (only if you disable bf16)

# Memory/perf
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# Logging / output
logging_steps: 50
save_steps: 500
save_total_limit: 2
output_dir: ./qwen3-4b-qlora
seed: 42
